{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîó LangChain + LlamaIndex Integration for RAG Pipelines\n",
    "\n",
    "This notebook demonstrates a **detailed integration of LangChain and LlamaIndex** for building a production-grade RAG pipeline.\n",
    "\n",
    "## Key Features:\n",
    "- Document loading & chunking\n",
    "- Embedding generation\n",
    "- Indexing with FAISS (via LangChain or LlamaIndex)\n",
    "- Query-time retrieval and LLM response generation\n",
    "\n",
    "üì¶ Required packages: `langchain`, `llama-index`, `faiss-cpu`, `openai`, `tiktoken`, `docx2txt`, `pypdf`, `python-dotenv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain llama-index faiss-cpu openai tiktoken docx2txt pypdf python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîê 1. Setup API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÑ 2. Load Documents (PDF, DOCX, or TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader, PyPDFLoader, UnstructuredWordDocumentLoader\n",
    "\n",
    "def load_documents(file_path):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        loader = UnstructuredWordDocumentLoader(file_path)\n",
    "    else:\n",
    "        loader = TextLoader(file_path)\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è 3. Chunk Documents Using LangChain Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents, chunk_size=500, chunk_overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 4. Create Vector Store with FAISS + OpenAI Embeddings (LangChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "def build_faiss_index(docs):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ 5. Query Using LangChain Retriever + LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def query_langchain_retrieval(db, query):\n",
    "    retriever = db.as_retriever()\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "    return qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü¶ô 6. Optionally Use LlamaIndex for Same Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "\n",
    "def llamaindex_workflow(folder_path):\n",
    "    reader = SimpleDirectoryReader(folder_path)\n",
    "    docs = reader.load_data()\n",
    "    embed_model = OpenAIEmbedding()\n",
    "    index = VectorStoreIndex.from_documents(docs, embed_model=embed_model)\n",
    "    query_engine = index.as_query_engine()\n",
    "    return query_engine.query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ End-to-End Example (LangChain)\n",
    "Load ‚Üí Chunk ‚Üí Index ‚Üí Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"./sample.pdf\"  # Replace with your own\n",
    "# docs = load_documents(file_path)\n",
    "# split_docs = split_documents(docs)\n",
    "# db = build_faiss_index(split_docs)\n",
    "# response = query_langchain_retrieval(db, \"What is the purpose of the agreement?\")\n",
    "# print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
