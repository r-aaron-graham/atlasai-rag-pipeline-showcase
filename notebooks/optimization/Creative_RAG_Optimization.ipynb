{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Creative RAG Pipeline Optimization\n",
    "\n",
    "This notebook explains the **core performance bottleneck** in RAG pipelines ‚Äî especially related to the trimming step ‚Äî and walks through a **creative, optimized, and cost-effective solution**.\n",
    "\n",
    "We will:\n",
    "- Identify the **issue** with the trimming step\n",
    "- Propose a **creative fix** using smart chunking and rehydration\n",
    "- Provide **code examples** with efficiency in mind\n",
    "- Include **tips to reduce cost and increase throughput**\n",
    "- Integrate optional **LLM-aware enhancements**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùó The Problem: Trimming Bottleneck\n",
    "\n",
    "The original pipeline used a **small model (e.g., o3-mini)** to trim retrieved documents before feeding them to GPT-4.\n",
    "- Trimming isolates content, hurting relevance.\n",
    "- Small models fail at \"needle-in-haystack\" tasks.\n",
    "- It creates a **major infrastructure bottleneck** and adds latency.\n",
    "\n",
    "### üö´ Key Issues:\n",
    "- Poor chunk boundaries = content chopped mid-thought.\n",
    "- Redundant compute = all retrieved chunks evaluated by a weak model.\n",
    "- Increased cost with limited gain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Solution: Trimming-Free, Chunk-Aware Design\n",
    "\n",
    "**Instead of trimming**, we apply:\n",
    "- Smarter ~250 token chunking (format-aware)\n",
    "- Context rehydration (add neighbors at retrieval)\n",
    "- Optional use of LLM to guide hard splits\n",
    "- Efficient batching and lightweight parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "# !pip install tiktoken python-docx openai\n",
    "import tiktoken\n",
    "from docx import Document\n",
    "import random, openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 1: Smart Chunking Function\n",
    "- Keeps chunks around 250 tokens\n",
    "- Merges small sections and splits large ones with awareness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, model=\"gpt-4\"):\n",
    "    return tiktoken.encoding_for_model(model).encode(text)\n",
    "\n",
    "def smart_chunk_docx(path, min_tokens=100, max_tokens=250):\n",
    "    doc = Document(path)\n",
    "    chunks, buffer = [], \"\"\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        if not para.text.strip():\n",
    "            continue\n",
    "        buffer += \" \" + para.text.strip()\n",
    "        tokens = tokenize(buffer)\n",
    "        if len(tokens) >= max_tokens:\n",
    "            chunks.append(buffer.strip())\n",
    "            buffer = \"\"\n",
    "    if buffer:\n",
    "        chunks.append(buffer.strip())\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÅ Step 2: Rehydration Logic\n",
    "Adds neighboring chunks to restore context. This is better than trimming as it keeps original semantic flow intact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rehydrate_chunks(indices, all_chunks):\n",
    "    final = []\n",
    "    for idx in indices:\n",
    "        chunk_group = []\n",
    "        if idx > 0:\n",
    "            chunk_group.append(all_chunks[idx - 1])\n",
    "        chunk_group.append(all_chunks[idx])\n",
    "        if idx + 1 < len(all_chunks):\n",
    "            chunk_group.append(all_chunks[idx + 1])\n",
    "        final.append(\"\\n\".join(chunk_group))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Step 3: Simple Citation Marking\n",
    "Use `<AtlasSource id=X>` tags instead of messy JSON citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_simple_citations(text):\n",
    "    return re.findall(r\"<AtlasSource id=(\\d+)>\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Step 4: Load-Balanced OpenAI Calls\n",
    "Distribute across multiple API keys for higher throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_keys = [\"API_KEY_1\", \"API_KEY_2\"]  # replace with real keys\n",
    "\n",
    "def load_balanced_query(prompt):\n",
    "    openai.api_key = random.choice(api_keys)\n",
    "    return openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Optimization Summary\n",
    "| Dimension       | Optimization                                 |\n",
    "|----------------|-----------------------------------------------|\n",
    "| **Cost**       | Remove trimming, fewer model hops             |\n",
    "| **Throughput** | No o3-mini bottleneck, load balance GPT calls |\n",
    "| **Efficiency** | Pre-chunking + rehydration avoids redundancy  |\n",
    "| **Quality**    | Better context preservation and citation      |\n",
    "| **Creativity** | LLM-aware chunking (future) + markup formats  |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
