{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline Optimization – Jupyter Walkthrough\n",
    "This notebook demonstrates how to:\n",
    "- Remove trimming\n",
    "- Chunk documents smartly (~250 tokens)\n",
    "- Rehydrate surrounding chunks\n",
    "- Use formatting/metadata for context-aware chunking\n",
    "- (Optional) Use an LLM for chunk splitting\n",
    "- Simplify citation output\n",
    "- Apply load balancing to OpenAI API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries (uncomment below if not already installed)\n",
    "# !pip install python-docx tiktoken openai\n",
    "import openai\n",
    "import tiktoken\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenizer Helper\n",
    "Function to convert text into OpenAI token format using `tiktoken`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    return enc.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Paragraph-Aware Chunking\n",
    "This function reads a `.docx` file and chunks content by paragraphs, aiming for 100–250 token chunks.\n",
    "Very long paragraphs are split; short ones are merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_paragraphs(doc_path, min_tokens=100, max_tokens=250):\n",
    "    doc = Document(doc_path)\n",
    "    chunks, buffer = [], \"\"\n",
    "    \n",
    "    for para in doc.paragraphs:\n",
    "        if not para.text.strip():\n",
    "            continue\n",
    "        buffer += \" \" + para.text.strip()\n",
    "        if len(tokenize(buffer)) >= min_tokens:\n",
    "            if len(tokenize(buffer)) > max_tokens:\n",
    "                sentences = buffer.split('. ')\n",
    "                chunk, current = \"\", []\n",
    "                for s in sentences:\n",
    "                    current.append(s)\n",
    "                    if len(tokenize('. '.join(current))) >= max_tokens:\n",
    "                        chunks.append('. '.join(current))\n",
    "                        current = []\n",
    "                buffer = '. '.join(current)\n",
    "            else:\n",
    "                chunks.append(buffer)\n",
    "                buffer = \"\"\n",
    "    if buffer:\n",
    "        chunks.append(buffer)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chunk Rehydration\n",
    "Function to include 1 chunk before and after each selected chunk index, for better context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rehydrate(retrieved_ids, all_chunks):\n",
    "    rehydrated = []\n",
    "    for i in retrieved_ids:\n",
    "        parts = [all_chunks[i]]\n",
    "        if i > 0:\n",
    "            parts.insert(0, all_chunks[i - 1])\n",
    "        if i + 1 < len(all_chunks):\n",
    "            parts.append(all_chunks[i + 1])\n",
    "        rehydrated.append(\" \".join(parts))\n",
    "    return rehydrated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simplified Citation Format\n",
    "Use this format in your prompts and outputs to simplify parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_citations(text):\n",
    "    return re.findall(r\"<AtlasSource id=(\\d+)>\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Balanced OpenAI API Call\n",
    "Randomly choose an API key per call for higher throughput across quotas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "api_keys = [\"API_KEY_1\", \"API_KEY_2\", \"API_KEY_3\"]\n",
    "\n",
    "def query_openai(prompt):\n",
    "    openai.api_key = random.choice(api_keys)\n",
    "    return openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
